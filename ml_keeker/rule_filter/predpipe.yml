# EVENT_LABEL-SUB_LABEL:
#   description: blahblah
#   pattern: "{abc} - {def}"
#   extract : True 
#     - 
#     - 

INIT-start_logger:
    description: "initialize logger"
    pattern:
        - "start logging"

INIT-print_args:
    description: "print out arguments when execute script"
    pattern: 
        - "arguments: {args_list}"

INIT-connect_kafka: 
    description: "connecting to kafka topic"
    pattern: 
      - "connecting to topic {kafka_topic} with config {config}"
      - "connecting to topic {kafka_topic} with config {config} pipeline_id:{pipeline_id}"
    extract: True

INIT-update_component:
    description: "update processing component"
    pattern: 
      - "updating processing components with max-latency {max_latency}"
      - "updating processing components with max-latency {max_latency} pipeline_id:{pipeline_id}"
    extract: True

INIT-model_spec:
    description: "running model spec"
    pattern: 
      - "running model of type {model_type} created at {time}"

EMPTYDATA: 
    description: "empty data from kafka topic"
    pattern: 
        - "empty data or no data pipeline_id:{pipeline_id}"
        - "empty data or no data"
        - "empty data"
        - "more than 45 minutes without data"

READ-start_read_msg:
    description: "start to read kafka message"
    pattern:
        - "starting reading Kafka stream at time {kafka_start}\\+{timezone} pipeline_id:{pipeline_id}"

READ-reading_kafka_msg:
    description: "reading kafka message. poll() - before_poll"
    pattern: 
        - "reading more than 1 Kafka msg at a time for ML to catch up; poll time: {duration}/{timeout}"
    extract: True

READ-finish_to_read:
    description: "finish to read messages"
    pattern: 
        - "worker just read {row_num} rows out of total: {all_row_num}"

READ-large_delay:
    description: "catching up message"
    pattern: 
      - "large delay of {minute} minutes. Keep buffering to catch up faster"
    extract: True

READ-poll_time:
    description: "randomly printed kafka poll time from kafka_reader. line 133"
    pattern: 
      - "kafka poll time: {duration}\\/{timeout}"
    extract: True

READ-expect_time:
    description: "estimate time"
    pattern:
        - "expected-time: {expected}; pushed: {pushed}; max-lat: {max_latency}; now-msg_time: {now_msg_time} pipeline_id:{pipeline_id}"
        - "expected-pred-duration: {expected}; pushed: {pushed}/{max_buffer_size}; eldest: {now_msg_time}/{max_latency}; loops: {loops}"

READ-pushed:
    description: ""
    pattern:
        - "pushed: {pushed}/{max_buffer_size}; eldest: {now_msg_time}/{max_latency}; loops: {loops}; mem: {memory_count}/{max_input_mem}; expired: {flush}"

WRITE-pred_file:
    description: "Write prediction file to HDFS"
    pattern:
        - "Writing to '{pred_tmp}'. pipeline_id:{pipeline_id} elastic:{elastic} docker_id:{docker_id}"
        - "writing predictions to {pred_tmp} pipeline_id:{pipeline_id}"
        - "writing {num_predicion} predictions to {pred_tmp}"

MOVE-pred_file:
    description: "Move prediction file to HDFS"
    pattern:
        - "moving {pred_tmp} to {pred_dir}"
        - "moving {pred_tmp} to {pred_dir} pipeline_id:{pipeline_id} elastic:{elastic} docker_id:{docker_id}"
        - "Renaming '{pred_tmp}' to '{pred_dir}'. pipeline_id:{pipeline_id} elastic:{elastic} docker_id:{docker_id}"

COMMIT-offset_partition:
    description: "Commit to kafka topic's partition"
    pattern:
        - "comitted Kafka offsets on partitions {partition_idx}"
        - "committed Kafka offsets on partitions {partition_idx}"
        - "committed partition:offset {kafka_offset} on Kafka pipeline_id:{pipeline_id} elastic:{elastic} docker_id:{docker_id}"
        - "comitted partition:offset {kafka_offset} on Kafka pipeline_id:{pipeline_id} elastic:{elastic} docker_id:{docker_id}"

PUBLISH-speed:
    description: "publish speed"
    pattern:
        - "publishing speed {speed} for pipeline {pipeline} pipeline_id:{pipeline_id2} elastic:{elastic} docker_id:{docker_id}"
        - "speed {speed} published for pipeline {pipeline} pipeline_id:{pipeline_id2} elastic:{elastic} docker_id:{docker_id}"

CREATE-ephemeral-speed:
    description: "create ephemeral node for storing speed"
    pattern:
        - "ephemeral {znode_path} created pipeline_id:{pipeline_id} elastic:{elastic} docker_id:{docker_id}"
        - "creating ephemeral prefixed by {znode_path} pipeline_id:{pipeline_id} elastic:{elastic} docker_id:{docker_id}"

CLOSE-clean_up:
    description: "client clean up"
    pattern: 
      - "gracefully cleaning up"
      - "gracefully cleaning up pipeline_id:{pipeline_id}"

CLOSE-close_reader:
    description: "close reader client"
    pattern:
        - "closing reader pipeline_id:{pipeline_id}"

CLOSE-close_sink:
    description: "close sink client"
    pattern:
        - "closing output sink pipeline_id:{pipeline_id}"

RESTART:
    description: "ready to exit and restart"
    pattern: 
      - "all services are closed. Ready to exit and restart."
      - "all services are closed. Ready to exit and restart. pipeline_id:{pipeline_id}"
      - "receive SIGINT (probably from Docker)"

TIMEOUT-no_data:
    description: "TimeoutError from process.py"
    pattern:
        - "more than {timeout} minutes without data pipeline_id:{pipeline_id}"

WARNING:
    description: "warning situation"
    pattern: 
      - "column {column_name} has inf value. Changed to NaN"
      - "column {column_name} has {percentage}% NaN. Changed to 0"
      - "exception caught at the top level pipeline_id:{pipeline_id} {traceback}"

ERROR:
    description: "error situation"
    pattern: 
      - "error in prediction engine>>{traceback}"
      - "exception caught at the top level pipeline_id:{pipeline_id} {traceback}"
    extract: True

SKIP:
    description: "skippable log event"
    pattern: 
      - "healthcheck: off"
      - "No feedback has been given"
